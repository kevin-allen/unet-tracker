{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761f4ae0",
   "metadata": {},
   "source": [
    "# Train U-Net\n",
    "\n",
    "This is very similar code to `11_unet_carvana.ipynb` but we have 2 classes instead of 1.\n",
    "\n",
    "With approximately 400 images in the training set, I trained for 100 epochs and get very good results for face tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e1ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.dataset import UNetDataset\n",
    "from unetTracker.unet import Unet\n",
    "from unetTracker.coordinatesFromSegmentationMask import CoordinatesFromSegmentationMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd6f0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/finger_tracker\n",
      "Getting configuration from config file. Values from config file will be used.\n",
      "Loading /home/kevin/Documents/trackingProjects/finger_tracker/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_extension': '.png', 'image_size': [270, 480], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'finger_tracker', 'normalization_values': {'means': [0.40835028886795044, 0.4549056589603424, 0.51627117395401], 'stds': [0.23996737599372864, 0.251758873462677, 0.26929107308387756]}, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (240.0, 255.0, 255.0)], 'objects': ['f1', 'f2', 'f3', 'f4'], 'target_radius': 6, 'unet_features': [64, 128, 256, 512]}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"finger_tracker\",root_folder = \"/home/kevin/Documents/trackingProjects/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e72331",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98657051",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-4\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")) \n",
    "BATCH_SIZE=3\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = 2\n",
    "OUTPUT_CHANNELS = len(project.object_list)\n",
    "IMAGE_HEIGHT = project.image_size[0]\n",
    "IMAGE_WIDTH = project.image_size[1]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMAGE_DIR = os.path.join(project.dataset_dir,\"train_images\")\n",
    "TRAIN_MASK_DIR =  os.path.join(project.dataset_dir,\"train_masks\")\n",
    "TRAIN_COORDINATE_DIR = os.path.join(project.dataset_dir,\"train_coordinates\")\n",
    "VAL_IMAGE_DIR = os.path.join(project.dataset_dir,\"val_images\")\n",
    "VAL_MASK_DIR =  os.path.join(project.dataset_dir,\"val_masks\")\n",
    "VAL_COORDINATE_DIR = os.path.join(project.dataset_dir,\"val_coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a4d20",
   "metadata": {},
   "source": [
    "## Model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b15df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/python_environments/dsenv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Unet(in_channels=3, out_channels=OUTPUT_CHANNELS).to(DEVICE)\n",
    "if LOAD_MODEL:\n",
    "    project.load_model(model)\n",
    "\n",
    "# set the model in train mode\n",
    "model.train()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # not doing sigmoid on the output of the model, so use this, if we had more classes (objects) we would use change out_chan and cross_entropy_loss as loss_fn\n",
    "optimizer= optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd0ca19-9df7-4808-b5aa-e3c92d874cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (downs): ModuleList(\n",
       "    (0): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (finalConv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c223b6f-3854-4b15-af22-3b0d87148b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 31037828\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41646fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data augmentation and normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdddb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/trainTransform\n",
      "Loading valTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/valTransform\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join(project.augmentation_dir,\"trainTransform\")\n",
    "print(\"Loading trainTransform from\", fileName)\n",
    "trainTransform=pickle.load(open(fileName,\"rb\" ))\n",
    "\n",
    "fileName = os.path.join(project.augmentation_dir,\"valTransform\")\n",
    "print(\"Loading valTransform from\", fileName)\n",
    "valTransform=pickle.load(open(fileName, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35254b",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb48e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = UNetDataset(TRAIN_IMAGE_DIR, TRAIN_MASK_DIR,TRAIN_COORDINATE_DIR, transform=trainTransform)\n",
    "valDataset = UNetDataset(VAL_IMAGE_DIR, VAL_MASK_DIR,VAL_COORDINATE_DIR, transform=valTransform)\n",
    "trainLoader = DataLoader(trainDataset,shuffle=True,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY)\n",
    "valLoader = DataLoader(valDataset,shuffle=False,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory = PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c074303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainLoader = DataLoader(trainDataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)\n",
    "valLoader = DataLoader(valDataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b6560e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 480, 640]), torch.Size([3, 4, 480, 640]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, masks, _ = next(iter(trainLoader))\n",
    "imgs.shape, masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6ceb",
   "metadata": {},
   "source": [
    "There is a lot of black because half of our pixels are below 0, on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd71a06",
   "metadata": {},
   "source": [
    "# Save and load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2448b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
    "    #print(\"Saving checkpoint\")\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4c8b8",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b20d7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model,loader,device):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    num_mask = 0\n",
    "    num_mask_detected = 0\n",
    "    num_detected = 0\n",
    "    sum_distance = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y,c in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            output = torch.sigmoid(model(x))\n",
    "            preds = (output > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2*(preds * y).sum() / ((preds+y).sum() + 1e-8)) # work only for binary\n",
    "\n",
    "            # proportion of the mask detected\n",
    "            num_mask += y.sum()\n",
    "            num_mask_detected += preds[y==1.0].sum()\n",
    "            num_detected += preds.sum()\n",
    "\n",
    "            # distance between predicted coordinates and labelled coordinates\n",
    "            output = output.detach().cpu().numpy()\n",
    "            pred_coords = cDetector.detect(output)\n",
    "\n",
    "            sum_distance+= np.nanmean(np.sqrt(((pred_coords[:,:,0:2] - c.numpy())**2).sum(axis=2)))\n",
    "            # we acutally do a mean of the error for the different objects in a batch\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(loader):.2f}\")\n",
    "    print(f\"Mask pixels detected: {num_mask_detected/num_mask*100:.2f}%\")\n",
    "    print(f\"False positives: {(num_detected-num_mask_detected)/num_detected*100:.2f}%\")\n",
    "    print(f\"Mean distance: {sum_distance/len(loader)}\")\n",
    "    a = model.train()\n",
    "\n",
    "cDetector = CoordinatesFromSegmentationMask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a27c99",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d567c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader,model,optimizer,loss_fn,scaler,epoch,total_epochs):\n",
    "    \"\"\"\n",
    "    One epoch of training\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data,targets,_) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE)\n",
    "        \n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions,targets)\n",
    "            \n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # update tqdm loop\n",
    "        loop.set_postfix_str(\"loss: {:.7f}, epoch: {:d}/{:d}\".format(loss.item(),epoch,total_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dbc7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time: 2022-12-18 17:56:27.394272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 213/213 [00:40<00:00,  5.32it/s, loss: 0.0003464, epoch: 0/2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.96\n",
      "Dice score: 0.81\n",
      "Mask pixels detected: 83.05%\n",
      "False positives: 18.85%\n",
      "Mean distance: 4.257743132655602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 213/213 [00:38<00:00,  5.48it/s, loss: 0.0026969, epoch: 1/2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2022-12-18 17:57:52.732212\n",
      "2 epochs, duration: 0:01:25.337940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "print(\"Starting time:\",startTime)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    train_fn(trainLoader,model,optimizer,loss_fn,scaler,epoch,NUM_EPOCHS)\n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint,filename=os.path.join(project.models_dir,\"my_checkpoint.pth.tar\"))\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(model,valLoader,DEVICE)\n",
    "\n",
    "endTime=datetime.now()\n",
    "print(\"End time:\",endTime)\n",
    "print(\"{} epochs, duration:\".format(NUM_EPOCHS), endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929bfc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model state dict to /home/kevin/Documents/trackingProjects/faceTrack/models/UNet.pt\n",
      "2022-12-18 17:57:52.737072\n"
     ]
    }
   ],
   "source": [
    "project.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c70b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
