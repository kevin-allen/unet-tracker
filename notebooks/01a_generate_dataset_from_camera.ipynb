{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract images from a live video to create a dataset\n",
    "\n",
    "We now use a simple GUI to generate new dataset for image segmentation. \n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "\n",
    "* Get an image from a camera (could later be a video or images). \n",
    "* Click on the video to extract one image, and then click again to label the oject on the image.\n",
    "* Save image, coordinate and mask to the dataset.\n",
    "* Split images and mask as a train and validation dataset.\n",
    "\n",
    "You want at least 150 imgaes to get ok tracking. We can further improve the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.dataset import UNetDataset\n",
    "from unetTracker.camera import USBCamera, bgr8_to_jpeg\n",
    "from unetTracker.unetGUI import LabelFromCameraGUI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a TrackingProject object from file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/faceTrack\n",
      "Getting configuration from config file. Values from config file will be used.\n",
      "Loading /home/kevin/Documents/trackingProjects/faceTrack/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.5, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_extension': '.png', 'image_size': [480, 640], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'faceTrack', 'normalization_values': None, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (240.0, 255.0, 255.0), (0.0, 128.0, 0.0), (169.0, 169.0, 169.0), (255.0, 20.0, 147.0)], 'objects': ['nose', 'eyeL', 'eyeR', 'chin', 'mouthL', 'mouthC', 'mouthR'], 'target_radius': 6, 'unet_features': [64, 128, 256, 512]}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"faceTrack\",root_folder = \"/home/kevin/Documents/trackingProjects/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kevin/Documents/trackingProjects/faceTrack/dataset/masks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.mask_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset object\n",
    "\n",
    "A Dataset object is created to save labeled images, masks and coordinates. It is also used to load the data into memory when training your model. This is a class inherited from `torch.utils.data.Dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UNetDataset(image_dir=project.image_dir, mask_dir=project.mask_dir, coordinate_dir=project.coordinate_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know how many images are in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load any of the images in your dataset, together with mask and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image,mask,coordinates \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/repo/unetTracker/unetTracker/dataset.py:110\u001b[0m, in \u001b[0;36mUNetDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,index):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Function to get an item from the dataset\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    You can set self.transform to process the item the when loaded. Use albumentation transform functions.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    111\u001b[0m     mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_extension,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_mask.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    112\u001b[0m     coordinates_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinate_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_extension,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_coordinates.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "image,mask,coordinates = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are getting an image, mask, and coordinates of objects.\n",
    "\n",
    "The image and mask are `torch.tensor` instead of `np.array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crw-rw----+ 1 root video 81, 1 Nov 14 21:15 /dev/video1\n",
      "crw-rw----+ 1 root video 81, 0 Nov 14 21:15 /dev/video0\n"
     ]
    }
   ],
   "source": [
    "!ls -ltrh /dev/video*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = USBCamera(width=project.image_size[1], height=project.image_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "image = camera.read()\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82451c69aa7e4ea2b2000c20c85ef90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgWidget = widgets.Image(format='jpeg')\n",
    "imgWidget.value = bgr8_to_jpeg(image)\n",
    "display(imgWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the GUI to extract and label frames from a camera \n",
    "\n",
    "1. Click on the top left image to capture a frame from the camera feed. A larger image will appear below the entry boxes for the coordinates. \n",
    "2. Your body parts are listed above the large image. In the large image, click on the object that is selected in the radio button. You should see a label apper in the picture at the very bottom.\n",
    "3. Repeat for all the body parts visible on the image. If you leave the coordinate of an object at 0,0, it is considered not in the image.\n",
    "4. Once all the body parts are labeled, click on `Save labelled frame`.\n",
    "5. Go back to step 1 and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLabelFromCameraGUI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/unetTracker/unetTracker/unetGUI.py:59\u001b[0m, in \u001b[0;36mLabelFromCameraGUI.__init__\u001b[0;34m(self, camera, project, dataset, model, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# image normalization\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m means \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalization_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeans\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m stds \u001b[38;5;241m=\u001b[39m project\u001b[38;5;241m.\u001b[39mnormalization_values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([A\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39mmeans, std\u001b[38;5;241m=\u001b[39mstds)])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "LabelFromCameraGUI(camera,project,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you click on `Save labelled frame`, a new data point is added to your dataset. This is stored in a subdirectory of your project directory called `dataset`. There are 3 directories there: images, masks and coordinates. This data will be used to train the network and assess tracking quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
