{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991a99e3-b732-4a61-a9e4-f636bffeafc2",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "\n",
    "Data augmentation is a process by which we modify our images and masks when we load them from files using the dataset Object. This process is stochastic, so the images loaded in each training epoch differ slightly.\n",
    "\n",
    "The main reason one uses data augmentation is to augment the generalization of our model to new images. By changing the images in each training epoch, we force the model to learn more generalizable features instead of learning specific features unique to each labeled image. \n",
    "\n",
    "\n",
    "I used the [Albumentations](https://albumentations.ai/) library to implement data augmentation. One can create a pipeline to process our images when they are loaded from files. The library can work with image-mask pairs used for segmentation. \n",
    "\n",
    "The normalization of our images (set mean and std of each channel to 0 and 1) can also be performed by the data augmentation pipeline. See the notebook on data normalization.\n",
    "\n",
    "Below, I am using four transformations. We can set the probability that this transformation is applied using the `p` argument. You can set it in the project configuration file. Alternatively, you can edit the code below.\n",
    "\n",
    "### Tips\n",
    "\n",
    "* If you track left/right body parts, you probably don't want to flip your images. This would mean setting 'augmentation_HorizontalFlipProb': 0.0 in your `config.yalm` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2e3b60-67ec-4f01-8c6b-53d1dfdf35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/finger_tracker\n",
      "Getting configuration from config file. Values from config file will be used.\n",
      "Loading /home/kevin/Documents/trackingProjects/finger_tracker/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_extension': '.png', 'image_size': [270, 480], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'finger_tracker', 'normalization_values': {'means': [0.40835028886795044, 0.4549056589603424, 0.51627117395401], 'stds': [0.23996737599372864, 0.251758873462677, 0.26929107308387756]}, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (240.0, 255.0, 255.0)], 'objects': ['f1', 'f2', 'f3', 'f4'], 'target_radius': 6, 'unet_features': [64, 128, 256, 512]}\n"
     ]
    }
   ],
   "source": [
    "# this will run the code in the setup_project.py and create a variable called `project`\n",
    "%run setup_project.py\n",
    "\n",
    "import albumentations as A\n",
    "from unetTracker.dataset import UNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bab07e-988e-45bc-9bd8-f6deea622ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose([\n",
      "  RandomSizedCrop(always_apply=False, p=1.0, min_max_height=(220, 270), height=270, width=480, w2h_ratio=1.7777777777777777, interpolation=1),\n",
      "  HorizontalFlip(always_apply=False, p=0.0),\n",
      "  Rotate(always_apply=False, p=0.3, limit=(-30, 30), interpolation=1, border_mode=0, value=None, mask_value=None, rotate_method='largest_box', crop_border=False),\n",
      "  RandomBrightnessContrast(always_apply=False, p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.40835028886795044, 0.4549056589603424, 0.51627117395401], std=[0.23996737599372864, 0.251758873462677, 0.26929107308387756], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n",
      "Compose([\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.40835028886795044, 0.4549056589603424, 0.51627117395401], std=[0.23996737599372864, 0.251758873462677, 0.26929107308387756], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    }
   ],
   "source": [
    "original_height = project.image_size[0]\n",
    "original_width = project.image_size[1]\n",
    "means = project.normalization_values[\"means\"]\n",
    "stds = project.normalization_values[\"stds\"]\n",
    "\n",
    "trainTransform = A.Compose([   \n",
    "                    A.RandomSizedCrop(min_max_height=(original_height-50, original_height),\n",
    "                                      w2h_ratio=original_width/original_height,height=original_height, width=original_width, p=project.augmentation_RandomSizedCropProb),\n",
    "                    A.HorizontalFlip(p=project.augmentation_HorizontalFlipProb),\n",
    "                    A.Rotate (limit=30,border_mode=cv2.BORDER_CONSTANT,p=project.augmentation_RotateProb),\n",
    "                    A.RandomBrightnessContrast(p=project.augmentation_RandomBrightnessContrastProb),\n",
    "                    A.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "valTransform = A.Compose([   \n",
    "                    A.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "\n",
    "print(trainTransform)\n",
    "print(valTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55810dbe-64c0-4656-8b3b-b4aee449ef17",
   "metadata": {},
   "source": [
    "We can save the transformation in a `augmentation` directory inside our project directory.\n",
    "\n",
    "We will be able to load the data augmentation pipeline from files when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c71ae73-2c64-476f-97ba-0728eda43267",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.augmentation_dir\n",
    "if os.path.exists(project.augmentation_dir) == False:\n",
    "    os.mkdir(project.augmentation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b454ae-8c08-4c54-881e-37e56f8dcd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trainTransform as /home/kevin/Documents/trackingProjects/finger_tracker/augmentation/trainTransform\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join(project.augmentation_dir,\"trainTransform\")\n",
    "print(\"Saving trainTransform as\", fileName)\n",
    "pickle.dump( trainTransform, open( fileName, \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a8edfc-8e5d-46cf-976b-4a4645213776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trainTransform as /home/kevin/Documents/trackingProjects/finger_tracker/augmentation/valTransform\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join(project.augmentation_dir,\"valTransform\")\n",
    "print(\"Saving trainTransform as\", fileName)\n",
    "pickle.dump( valTransform, open( fileName, \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c0e939-2940-45d0-9e29-394a9780e186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
